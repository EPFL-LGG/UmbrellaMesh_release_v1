{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "import elastic_rods, sparse_matrices, pickle, scipy, linkage_vis, numpy as np, time\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from numpy.linalg import norm\n",
    "from io_redirection import suppress_stdout\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "elastic_rods.set_max_num_tbb_threads(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatLinkage = pickle.load(open('../data/opt_test_flat.pkl', 'rb'))\n",
    "deployedLinkage = pickle.load(open('../data/opt_test_deployed.pkl', 'rb'))\n",
    "perturbation_dir = np.load('../data/opt_test_perturbation_dir.npy')\n",
    "joint_target_perturbation = np.load('../data/opt_test_joint_target_perturb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eopts = elastic_rods.NewtonOptimizerOptions()\n",
    "eopts.verbose = True\n",
    "eopts.niter = 50\n",
    "params = flatLinkage.getPerSegmentRestLength()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eopts.gradTol = 1e-13\n",
    "lopt = elastic_rods.LinkageOptimization(flatLinkage, deployedLinkage, eopts)\n",
    "lopt.joint_pos_tgt = lopt.joint_pos_tgt + joint_target_perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lopt.beta = 0\n",
    "lopt.gamma = 0.0\n",
    "analytic_gradp_J_target = np.dot(lopt.gradp_J_target(params), perturbation_dir)\n",
    "analytic_gradp_J = np.dot(lopt.gradp_J(params), perturbation_dir)\n",
    "\n",
    "def derivative_error(eps = 1e-8, targetOnly = False):\n",
    "    perturbation = eps * perturbation_dir\n",
    "    fd, analytic = None, None\n",
    "    if targetOnly:\n",
    "        fd = (lopt.J_target(params + perturbation) - lopt.J_target(params - perturbation)) / (2 * eps)\n",
    "        return (fd - analytic_gradp_J_target) / analytic_gradp_J_target\n",
    "    else:\n",
    "        fd = (lopt.J(params + perturbation) - lopt.J(params - perturbation)) / (2 * eps)\n",
    "        return (fd - analytic_gradp_J) / abs(analytic_gradp_J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting beta = 0 (considering only the elastic energy terms of the objective), finite difference derivatives converge nicely to the analytic gradients, getting ~8 digits of accuracy before roundoff error dominates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "epsilons = np.power(10, np.linspace(-4,-9, 30))\n",
    "errors = [derivative_error(eps) for eps in epsilons]\n",
    "plt.loglog(epsilons, np.abs(errors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture is more complicated for the target-fitting term: while the error is low, it's not obvious that the finite difference approximation is converging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = np.power(10, np.linspace(-4,-9, 100))\n",
    "errors = [derivative_error(eps, targetOnly=True) for eps in epsilons]\n",
    "plt.loglog(epsilons, np.abs(errors))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the target-fitting term value (instead of gradient), we see the numerically evaluated objective is discontinuous and highly sensitive to the gradient tolerance used during the equilibrium solve. This is to be expected: if we don't solve for the equilibrium displacement with enough accuracy, the target-fitting objective won't be computed accurately enough for finite differencing.\n",
    "\n",
    "In particular, we notice a discontinuity in the scatter plot below, which shows the fitting objective term evaluated at multiple steps along the design parameter perturbation vector. Essentially, this discontinuity can be traced back to the decision on whether another newton step is needed. The discontinuous blue curve is generated by always starting the equilibrium solve from the original design's equilibrium. If we instead continuously update the initial guess for the equilibrium as we step along the perturbation vector (orange curve), the equilibrium is solved quite accurately at each step with just a single newton step (making gradTol irrelevant), and the discontinuity disappears. The discontinuity can also be mitigated by reducing gradTol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eopts.gradTol = 1e-11\n",
    "eopts.verbose = 10\n",
    "lopt = elastic_rods.LinkageOptimization(flatLinkage, deployedLinkage, eopts)\n",
    "lopt.joint_pos_tgt = lopt.joint_pos_tgt + joint_target_perturbation\n",
    "\n",
    "def runTest(epsilons, updateInit = False):\n",
    "    Jfit = []\n",
    "    energy = []\n",
    "    for eps in epsilons:\n",
    "        if updateInit: lopt.newPt(params + eps * perturbation_dir)\n",
    "        Jfit.append(lopt.J_target(params + eps * perturbation_dir))\n",
    "        energy.append(lopt.getLinesearchDeployedLinkage().energy())\n",
    "    return (epsilons, Jfit, energy)\n",
    "fixed_init = runTest(np.linspace(0, 1e-6, 50))\n",
    "updated_init = runTest(np.linspace(0, 1e-6, 50), True)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "plt.xlim(min(fixed_init[0]), max(fixed_init[0]))\n",
    "plt.ylim(min(fixed_init[1]), max(fixed_init[1]))\n",
    "plt.scatter(fixed_init[0], fixed_init[1], label=\"fixed_init\")\n",
    "plt.scatter(updated_init[0], updated_init[1], marker=\".\", label=\"updated_init\")\n",
    "plt.show()\n",
    "# plt.xlim(min(fixed_init[0]), max(fixed_init[0]))\n",
    "# plt.ylim(min(fixed_init[2]), max(fixed_init[2]))\n",
    "# plt.scatter(fixed_init[0], fixed_init[2], label=\"fixed_init\")\n",
    "# plt.scatter(updated_init[0], updated_init[2], marker=\".\", label=\"updated_init\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
